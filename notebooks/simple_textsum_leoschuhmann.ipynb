{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_textsum_leoschuhmann.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mJWm35uhZZ3"
      },
      "source": [
        "# **Automatic Text Summarizer and Metadata extractor with JSON Output**\n",
        "## *Leo Schuhmann*\n",
        "This notebook takes an english PDF-File as input, extracts its available Metadata and reads its text. <br> After that different ways of generating a summary with keywords and abstractive and extractive techniques get used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJrP_0W5h4hy"
      },
      "source": [
        "#**Please install following libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n7jj83Ch_rR"
      },
      "source": [
        "!pip install --upgrade pyPDF2\n",
        "!pip install --upgrade sentencepiece\n",
        "!pip install --upgrade bert-extractive-summarizer\n",
        "!pip install --upgrade spacy\n",
        "!pip install --upgrade transformers\n",
        "!pip install --upgrade neuralcoref\n",
        "!pip install --upgrade pegasuspy\n",
        "!python -m spacy download en_core_web_md\n",
        "!pip install --upgrade git+https://github.com/google/flax.git\n",
        "!pip install --upgrade python-rake\n",
        "!pip install --upgrade nltk\n",
        "!pip install --upgrade torch\n",
        "!pip install --upgrade re\n",
        "!pip install ipywidgets\n",
        "!pip install IPython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_GyucvgkiR9"
      },
      "source": [
        "#**Import necessary libraries and download helper files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDKxtAtGiUBh"
      },
      "source": [
        "from ipywidgets import FileUpload\n",
        "from IPython.display import display\n",
        "from PyPDF2 import PdfFileReader\n",
        "import nltk\n",
        "import re\n",
        "from summarizer import Summarizer\n",
        "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration, PegasusTokenizer, PegasusForConditionalGeneration, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "import pprint\n",
        "import RAKE\n",
        "import pprint\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "pp = pprint.PrettyPrinter(indent=4) #pretty print outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Loi7GZXXlgWq"
      },
      "source": [
        "#**Set the ENGLISH PDF file**\n",
        "*Please press on upload and select the PDF file to get started.* <br>\n",
        "**After executing the cell below and uploading the file only execute the next cells, not this one again.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqoZj_cb59IE"
      },
      "source": [
        "upload = FileUpload(accept='.pdf', multiple=False)\n",
        "display(upload)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74iIjaQ9uebS"
      },
      "source": [
        "*this code is converting the pdf upload file, to make it usable in the next steps*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGYZ6V6tne9Y"
      },
      "source": [
        "with open('file_output.pdf', 'wb') as output_file: \n",
        "    for uploaded_filename in upload.value:\n",
        "        content = upload.value[uploaded_filename]['content']   \n",
        "        output_file.write(content) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3Chl9M-nmU7"
      },
      "source": [
        "# **Now we can start**\n",
        "## 1. Read PDF Metadata and Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVzMruJiVSeh"
      },
      "source": [
        "def get_info(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        pdf = PdfFileReader(f)\n",
        "        info = pdf.getDocumentInfo()\n",
        "        number_of_pages = pdf.getNumPages()\n",
        "\n",
        "        full_text = []\n",
        "        for i in range(number_of_pages):\n",
        "          full_text.append((pdf.getPage(i)).extractText())\n",
        "        full_text = \" \".join(full_text)\n",
        "        \n",
        "    return full_text, info, number_of_pages\n",
        "\n",
        "full_text, metadata, number_of_pages = get_info('file_output.pdf')\n",
        "meta = metadata.copy()\n",
        "meta[r'/NumberPages'] = number_of_pages\n",
        "pp.pprint(meta)\n",
        "pp.pprint(full_text[:100]) #lets only view the first 100 chars "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTA7M86_p_vH"
      },
      "source": [
        "## 2. Basic Text Preperation and Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPOg6FKlVpAV"
      },
      "source": [
        "body = full_text.replace(\"/[^A-Za-z0-9\\s!?]/g\",'').replace(\"\\n\", \" \").strip()\n",
        "body = re.sub(' +', ' ', body)\n",
        "pp.pprint(body[:100]) #lets only view the first 100 chars "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITI8yYlurSzT"
      },
      "source": [
        "## 3. BERT Extractive Summarizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Available options and parameter:\n",
        "\n",
        "model = Summarizer(\n",
        "    **model**: This gets used by the hugging face bert library to load the model, you can supply a custom trained model here\n",
        "    **custom_model**: If you have a pre-trained model, you can add the model class here.\n",
        "    **custom_tokenizer**:  If you have a custom tokenizer, you can add the tokenizer here.\n",
        "    **hidden**: Needs to be negative, but allows you to pick which layer you want the embeddings to come from.\n",
        "    **reduce_option**: It can be 'mean', 'median', or 'max'. This reduces the embedding layer for pooling.\n",
        "    **sentence_handler**: The handler to process sentences. If want to use coreference, instantiate and pass CoreferenceHandler instance\n",
        ")\n",
        "\n",
        "model(\n",
        "    **body**: str # The string body that you want to summarize\n",
        "    **ratio**: float # The ratio of sentences that you want for the final summary\n",
        "    **min_length**: int # Parameter to specify to remove sentences that are less than min length characters\n",
        "    **max_length**: int # Parameter to specify to remove sentences greater than the max length,\n",
        "    **num_sentences**: Number of sentences to use. Overrides ratio if supplied.\n",
        ")\n",
        "\n",
        "**My tests showed that leaving the default parameters except for the output with num_sentences generally yields the best results**\n"
      ],
      "metadata": {
        "id": "FqwQDtv_SHhI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw937YNfW-eU"
      },
      "source": [
        "model = Summarizer()\n",
        "result = model(body, num_sentences=3) \n",
        "pp.pprint(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBZADpx-sI9x"
      },
      "source": [
        "## 4. T5 Model\n",
        "for fine-tuning see huggingface documentation: <br>\n",
        "https://huggingface.co/docs/transformers/main_classes/model\n",
        "\n",
        "https://huggingface.co/docs/transformers/main_classes/tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "\n",
        "t5_prepared_Text = \"summarize: \"+result\n",
        "tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\")\n",
        "\n",
        "summary_ids = model.generate(tokenized_text,\n",
        "                                    min_length=30,\n",
        "                                    max_length=100)\n",
        "\n",
        "output = ([tokenizer.decode(g, skip_special_tokens=True) for g in summary_ids])[0]\n",
        "pp.pprint(output)"
      ],
      "metadata": {
        "id": "5Hmo7SVwgwSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Input and Output Tensor generated:"
      ],
      "metadata": {
        "id": "6Q5wfiRbie8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pp.pprint(tokenized_text)\n",
        "\n",
        "pp.pprint(summary_ids)"
      ],
      "metadata": {
        "id": "g5K4m_f5gp1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUV-M1UJsu2v"
      },
      "source": [
        "## 5. Pegasus Model\n",
        "for fine-tuning see huggingface documentation: <br>\n",
        "https://huggingface.co/docs/transformers/main_classes/model\n",
        "\n",
        "https://huggingface.co/docs/transformers/main_classes/tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czXVLa0Ss0PQ"
      },
      "source": [
        "model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-xsum')\n",
        "tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')\n",
        "\n",
        "tokenized_text = tokenizer.encode(result, return_tensors='pt')\n",
        "\n",
        "summary_ids = model.generate(tokenized_text)\n",
        "out = ([tokenizer.decode(g, skip_special_tokens=True) for g in summary_ids])[0]\n",
        "pp.pprint(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txGcZNMzsVSO"
      },
      "source": [
        "## 6. Keywords: RAKE\n",
        "the parameters used here are the only ones available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yny1qwy9KdPN"
      },
      "source": [
        "rake = RAKE.Rake(RAKE.SmartStopList())\n",
        "keywords = []\n",
        "\n",
        "out_key = rake.run(body, minCharacters = 1, maxWords = 3, minFrequency = 3)\n",
        "if not out_key:\n",
        "  out_key = rake.run(body, minCharacters = 1, maxWords = 3, minFrequency = 2)\n",
        "  if not out_key:\n",
        "    out_key = rake.run(body, minCharacters = 1, maxWords = 3, minFrequency = 1)\n",
        "\n",
        "for entry in range(3):\n",
        "  keywords.append((out_key[entry][0]))\n",
        "\n",
        "pp.pprint(keywords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjXLoSKvtCKA"
      },
      "source": [
        "## Generate JSON Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvTWZKX-QQqj"
      },
      "source": [
        "json_result = {\"metadata\": meta, \n",
        "               \"BERT_extractive_sum\": result,\n",
        "               \"T5_abstractive_sum\": output,\n",
        "               \"Pegasus_abstractive_sum\": out,\n",
        "               \"Rake_top_3_keywords\": keywords}\n",
        "pp.pprint(json_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "N83dja3YwNfP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
